<?xml version="1.0"?>
<doc>
    <assembly>
        <name>Gpt4All</name>
    </assembly>
    <members>
        <member name="T:Gpt4All.Bindings.ILLModel">
            <summary>
            Represents the interface exposed by the universal wrapper for GPT4All language models built around llmodel C-API.
            </summary>
        </member>
        <member name="T:Gpt4All.Bindings.ModelResponseEventArgs">
            <summary>
            Arguments for the response processing callback
            </summary>
            <param name="TokenId">The token id of the response</param>
            <param name="Response"> The response string. NOTE: a token_id of -1 indicates the string is an error string</param>
            <return>
            A bool indicating whether the model should keep generating
            </return>
        </member>
        <member name="M:Gpt4All.Bindings.ModelResponseEventArgs.#ctor(System.Int32,System.String)">
            <summary>
            Arguments for the response processing callback
            </summary>
            <param name="TokenId">The token id of the response</param>
            <param name="Response"> The response string. NOTE: a token_id of -1 indicates the string is an error string</param>
            <return>
            A bool indicating whether the model should keep generating
            </return>
        </member>
        <member name="P:Gpt4All.Bindings.ModelResponseEventArgs.TokenId">
            <summary>The token id of the response</summary>
        </member>
        <member name="P:Gpt4All.Bindings.ModelResponseEventArgs.Response">
            <summary> The response string. NOTE: a token_id of -1 indicates the string is an error string</summary>
        </member>
        <member name="T:Gpt4All.Bindings.ModelPromptEventArgs">
            <summary>
            Arguments for the prompt processing callback
            </summary>
            <param name="TokenId">The token id of the prompt</param>
            <return>
            A bool indicating whether the model should keep processing
            </return>
        </member>
        <member name="M:Gpt4All.Bindings.ModelPromptEventArgs.#ctor(System.Int32)">
            <summary>
            Arguments for the prompt processing callback
            </summary>
            <param name="TokenId">The token id of the prompt</param>
            <return>
            A bool indicating whether the model should keep processing
            </return>
        </member>
        <member name="P:Gpt4All.Bindings.ModelPromptEventArgs.TokenId">
            <summary>The token id of the prompt</summary>
        </member>
        <member name="T:Gpt4All.Bindings.ModelRecalculatingEventArgs">
            <summary>
            Arguments for the recalculating callback
            </summary>
            <param name="IsRecalculating"> whether the model is recalculating the context.</param>
            <return>
            A bool indicating whether the model should keep generating
            </return>
        </member>
        <member name="M:Gpt4All.Bindings.ModelRecalculatingEventArgs.#ctor(System.Boolean)">
            <summary>
            Arguments for the recalculating callback
            </summary>
            <param name="IsRecalculating"> whether the model is recalculating the context.</param>
            <return>
            A bool indicating whether the model should keep generating
            </return>
        </member>
        <member name="P:Gpt4All.Bindings.ModelRecalculatingEventArgs.IsRecalculating">
            <summary> whether the model is recalculating the context.</summary>
        </member>
        <member name="T:Gpt4All.Bindings.LLModel">
            <summary>
            Base class and universal wrapper for GPT4All language models built around llmodel C-API.
            </summary>
        </member>
        <member name="M:Gpt4All.Bindings.LLModel.Create(System.IntPtr,Microsoft.Extensions.Logging.ILogger)">
            <summary>
            Create a new model from a pointer
            </summary>
            <param name="handle">Pointer to underlying model</param>
        </member>
        <member name="M:Gpt4All.Bindings.LLModel.Prompt(System.String,Gpt4All.Bindings.LLModelPromptContext,System.Func{Gpt4All.Bindings.ModelPromptEventArgs,System.Boolean},System.Func{Gpt4All.Bindings.ModelResponseEventArgs,System.Boolean},System.Func{Gpt4All.Bindings.ModelRecalculatingEventArgs,System.Boolean},System.Threading.CancellationToken)">
            <summary>
            Generate a response using the model
            </summary>
            <param name="text">The input promp</param>
            <param name="context">The context</param>
            <param name="promptCallback">A callback function for handling the processing of prompt</param>
            <param name="responseCallback">A callback function for handling the generated response</param>
            <param name="recalculateCallback">A callback function for handling recalculation requests</param>
            <param name="cancellationToken"></param>
        </member>
        <member name="M:Gpt4All.Bindings.LLModel.SetThreadCount(System.Int32)">
            <summary>
             Set the number of threads to be used by the model.
            </summary>
            <param name="threadCount">The new thread count</param>
        </member>
        <member name="M:Gpt4All.Bindings.LLModel.GetThreadCount">
            <summary>
            Get  the number of threads used by the model.
            </summary>
            <returns>the number of threads used by the model</returns>
        </member>
        <member name="M:Gpt4All.Bindings.LLModel.GetStateSizeBytes">
            <summary>
            Get the size of the internal state of the model.
            </summary>
            <remarks>
            This state data is specific to the type of model you have created.
            </remarks>
            <returns>the size in bytes of the internal state of the model</returns>
        </member>
        <member name="M:Gpt4All.Bindings.LLModel.SaveStateData(System.Byte*)">
            <summary>
            Saves the internal state of the model to the specified destination address.
            </summary>
            <param name="source">A pointer to the src</param>
            <returns>The number of bytes copied</returns>
        </member>
        <member name="M:Gpt4All.Bindings.LLModel.RestoreStateData(System.Byte*)">
            <summary>
            Restores the internal state of the model using data from the specified address.
            </summary>
            <param name="destination">A pointer to destination</param>
            <returns>the number of bytes read</returns>
        </member>
        <member name="M:Gpt4All.Bindings.LLModel.IsLoaded">
            <summary>
            Check if the model is loaded.
            </summary>
            <returns>true if the model was loaded successfully, false otherwise.</returns>
        </member>
        <member name="M:Gpt4All.Bindings.LLModel.Load(System.String)">
            <summary>
            Load the model from a file.
            </summary>
            <param name="modelPath">The path to the model file.</param>
            <returns>true if the model was loaded successfully, false otherwise.</returns>
        </member>
        <member name="T:Gpt4All.Bindings.LLModelPromptContext">
            <summary>
            Wrapper around the llmodel_prompt_context structure for holding the prompt context.
            </summary>
            <remarks>
            The implementation takes care of all the memory handling of the raw logits pointer and the
            raw tokens pointer.Attempting to resize them or modify them in any way can lead to undefined behavior
            </remarks>
        </member>
        <member name="P:Gpt4All.Bindings.LLModelPromptContext.Logits">
            <summary>
            logits of current context
            </summary>
        </member>
        <member name="P:Gpt4All.Bindings.LLModelPromptContext.LogitsSize">
            <summary>
            the size of the raw logits vector
            </summary>
        </member>
        <member name="P:Gpt4All.Bindings.LLModelPromptContext.Tokens">
            <summary>
            current tokens in the context window
            </summary>
        </member>
        <member name="P:Gpt4All.Bindings.LLModelPromptContext.TokensSize">
            <summary>
            the size of the raw tokens vector
            </summary>
        </member>
        <member name="P:Gpt4All.Bindings.LLModelPromptContext.TopK">
            <summary>
            top k logits to sample from
            </summary>
        </member>
        <member name="P:Gpt4All.Bindings.LLModelPromptContext.TopP">
            <summary>
            nucleus sampling probability threshold
            </summary>
        </member>
        <member name="P:Gpt4All.Bindings.LLModelPromptContext.Temperature">
            <summary>
            temperature to adjust model's output distribution
            </summary>
        </member>
        <member name="P:Gpt4All.Bindings.LLModelPromptContext.PastNum">
            <summary>
            number of tokens in past conversation
            </summary>
        </member>
        <member name="P:Gpt4All.Bindings.LLModelPromptContext.Batches">
            <summary>
            number of predictions to generate in parallel
            </summary>
        </member>
        <member name="P:Gpt4All.Bindings.LLModelPromptContext.TokensToPredict">
            <summary>
            number of tokens to predict
            </summary>
        </member>
        <member name="P:Gpt4All.Bindings.LLModelPromptContext.RepeatPenalty">
            <summary>
            penalty factor for repeated tokens
            </summary>
        </member>
        <member name="P:Gpt4All.Bindings.LLModelPromptContext.RepeatLastN">
            <summary>
            last n tokens to penalize
            </summary>
        </member>
        <member name="P:Gpt4All.Bindings.LLModelPromptContext.ContextSize">
            <summary>
            number of tokens possible in context window
            </summary>
        </member>
        <member name="P:Gpt4All.Bindings.LLModelPromptContext.ContextErase">
            <summary>
            percent of context to erase if we exceed the context window
            </summary>
        </member>
        <member name="T:Gpt4All.Bindings.NativeTypeNameAttribute">
            <summary>Defines the type of a member as it was used in the native signature.</summary>
        </member>
        <member name="M:Gpt4All.Bindings.NativeTypeNameAttribute.#ctor(System.String)">
            <summary>Initializes a new instance of the <see cref="T:Gpt4All.Bindings.NativeTypeNameAttribute" /> class.</summary>
            <param name="name">The name of the type that was used in the native signature.</param>
        </member>
        <member name="P:Gpt4All.Bindings.NativeTypeNameAttribute.Name">
            <summary>Gets the name of the type that was used in the native signature.</summary>
        </member>
        <member name="P:Gpt4All.Gpt4All.PromptFormatter">
            <inheritdoc/>
        </member>
        <member name="M:Gpt4All.LibraryLoader.NativeLibraryLoader.SetLibraryLoader(Gpt4All.LibraryLoader.ILibraryLoader)">
            <summary>
            Sets the library loader used to load the native libraries. Overwrite this only if you want some custom loading.
            </summary>
            <param name="libraryLoader">The library loader to be used.</param>
        </member>
        <member name="P:Gpt4All.IGpt4AllModel.PromptFormatter">
            <summary>
            The prompt formatter used to format the prompt before
            feeding it to the model, if null no transformation is applied
            </summary>
        </member>
        <member name="T:Gpt4All.IPromptFormatter">
            <summary>
            Formats a prompt
            </summary>
        </member>
        <member name="M:Gpt4All.IPromptFormatter.FormatPrompt(System.String)">
            <summary>
            Format the provided prompt
            </summary>
            <param name="prompt">the input prompt</param>
            <returns>The formatted prompt</returns>
        </member>
        <member name="T:Gpt4All.ITextPrediction">
            <summary>
            Interface for text prediction services
            </summary>
        </member>
        <member name="M:Gpt4All.ITextPrediction.GetPredictionAsync(System.String,Gpt4All.PredictRequestOptions,System.Threading.CancellationToken)">
            <summary>
            Get prediction results for the prompt and provided options.
            </summary>
            <param name="text">The text to complete</param>
            <param name="opts">The prediction settings</param>
            <param name="cancellation">The <see cref="T:System.Threading.CancellationToken"/> for cancellation requests. The default is <see cref="P:System.Threading.CancellationToken.None"/>.</param>
            <returns>The prediction result generated by the model</returns>
        </member>
        <member name="M:Gpt4All.ITextPrediction.GetStreamingPredictionAsync(System.String,Gpt4All.PredictRequestOptions,System.Threading.CancellationToken)">
            <summary>
            Get streaming prediction results for the prompt and provided options.
            </summary>
            <param name="text">The text to complete</param>
            <param name="opts">The prediction settings</param>
            <param name="cancellationToken">The <see cref="T:System.Threading.CancellationToken"/> for cancellation requests. The default is <see cref="P:System.Threading.CancellationToken.None"/>.</param>
            <returns>The prediction result generated by the model</returns>
        </member>
    </members>
</doc>
